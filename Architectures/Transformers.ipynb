{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I32e8X2YlzAk"
   },
   "source": [
    "# Named Entity Recognition using Transformers\n",
    "\n",
    "Bases on https://keras.io/examples/nlp/ner_transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/nlp/ner_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1639778008542,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "ZLj2de-KlzA0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fPXheWzlzA2"
   },
   "source": [
    "We will be using the transformer implementation from this fantastic\n",
    "[example](https://keras.io/examples/nlp/text_classification_with_transformer/).\n",
    "\n",
    "Let's start by defining a `TransformerBlock` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1639778009400,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "9Smj-VR_lzA4"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPnQWnyvlzA6"
   },
   "source": [
    "Next, let's define a `TokenAndPositionEmbedding` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1639778011158,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "JrrTIgqvlzA8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjpnlgEdlzA_"
   },
   "source": [
    "## Build the NER model class as a `keras.Model` subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1639778013270,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "Cr3HMIJTlzBA"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFVbmOvTlzBE"
   },
   "source": [
    "## Make the NER label lookup table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1639778018942,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "eLfwK2rVlzBF",
    "outputId": "c66fe94a-5eec-464b-de3e-ee1d7ff40dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'None', 2: 'Service', 3: 'Resource', 4: 'Device'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def make_tag_lookup_table():\n",
    "#     iob_labels = [\"B\", \"I\"]\n",
    "#     ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "#     all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "#     all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "#     all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "#     return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "mapping={0: '[PAD]', 1: 'None', 2:'Service',  3:'Resource', 4:'Device'}\n",
    "# mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1639778020987,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "Hj-Wimm9RIl2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 19:17:01.304062: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.TextLineDataset(\"train_dt_nf.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"val_dt_nf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3004,
     "status": "ok",
     "timestamp": 1639778030716,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "2S2xkvnyR2v-",
    "outputId": "58b402c5-c600-4478-b6d6-36d37010a00f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cristovao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13123\n"
     ]
    }
   ],
   "source": [
    "raw_train = np.load('TRAIN_DATA_np.npy', allow_pickle=True)\n",
    "raw_test = np.load('TEST_DATA_np.npy', allow_pickle=True)\n",
    "\n",
    "all_tokens = set()\n",
    "for instance in raw_train:\n",
    "    tokens=nltk.word_tokenize(instance[0])\n",
    "    for token in tokens:\n",
    "      all_tokens.add(token)\n",
    "\n",
    "\n",
    "for instance in raw_test:\n",
    "    tokens=nltk.word_tokenize(instance[0])\n",
    "    for token in tokens:\n",
    "      all_tokens.add(token)\n",
    "\n",
    "print(len(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzCry8aglzBG"
   },
   "source": [
    "Get a list of all tokens in the training dataset. This will be used to create the\n",
    "vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1639778032213,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "4HOuf_dLTUPW",
    "outputId": "9b48a888-e902-40a0-a1ed-bdd0569a0b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12072\n",
      "11998\n"
     ]
    }
   ],
   "source": [
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 12000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "print(len(vocabulary))\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(\n",
    "    vocabulary=vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vG0H03PlzBH"
   },
   "source": [
    "Create 2 new `Dataset` objects from the training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUFNfmPklzBI"
   },
   "source": [
    "Print out one line to make sure it looks good. The first record in the line is the number of tokens.\n",
    "After that we will have all the tokens followed by all the ner tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1639778036545,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "b35UIweIlzBM",
    "outputId": "811b29d4-b4d1-4355-cfa5-d017fb697eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'23\\tIt\\tlays\\ta\\tcertain\\ttheoretical\\tfoundation\\tfor\\tgiant\\tmagnetostrictive\\trelay\\tactuator\\tbeing\\tused\\tin\\tthe\\tfields\\tof\\tcutting\\twith\\tinvariableness\\tcutting\\tforce\\t.\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t3\\t3\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0', b\"20\\tHe\\t's\\tthe\\trepository\\tof\\tour\\tcommon\\thistory\\t,\\tand\\tby\\tthat\\tright\\t,\\tgrand\\tpatron\\tof\\tthe\\tBicentennial\\t.\\t0\\t0\\t0\\t2\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\"]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.take(2).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqX9AWVclzBN"
   },
   "source": [
    "We will be using the following map function to transform the data in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1639778037925,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "vKzgubzdlzBN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    # print(record)\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "\n",
    "# We use `padded_batch` here because each record in the dataset has a\n",
    "# different length.\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(32)\n",
    ")\n",
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1639778039922,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "UcDOJULdBtaZ",
    "outputId": "7288807f-f96d-4c9f-d5b1-c339866f9fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 1010,  7107,  3021, ...,     0,     0,     0],\n",
      "       [  519,  1027,  9466, ...,     0,     0,     0],\n",
      "       [  375,   519,  2540, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 1032,  1017,   417, ...,     0,     0,     0],\n",
      "       [  519,   799,   955, ...,     0,     0,     0],\n",
      "       [  519, 11936,   937, ...,     0,     0,     0]]), array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 3, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 3, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(val_dataset.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCtSLZunlzBO"
   },
   "source": [
    "We will be using a custom loss function that will ignore the loss from padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1639778042002,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "B1Be5eS6lzBO"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c84LQQ8zlzBP"
   },
   "source": [
    "## Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12760,
     "status": "ok",
     "timestamp": 1639778056343,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "o8KQvKr8Vtjv",
    "outputId": "1c671078-4d90-4d7f-dd7f-992c22011102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristovao/.virtualenvs/csi5137_course/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 5s 19ms/step - loss: 0.1764\n",
      "Epoch 2/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0474\n",
      "Epoch 3/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0335\n",
      "Epoch 4/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0255\n",
      "Epoch 5/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0224\n",
      "Epoch 6/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0186\n",
      "Epoch 7/10\n",
      "151/151 [==============================] - 3s 18ms/step - loss: 0.0171\n",
      "Epoch 8/10\n",
      "151/151 [==============================] - 3s 17ms/step - loss: 0.0142\n",
      "Epoch 9/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0120\n",
      "Epoch 10/10\n",
      "151/151 [==============================] - 2s 16ms/step - loss: 0.0103\n"
     ]
    }
   ],
   "source": [
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)\n",
    "\n",
    "\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1639778062459,
     "user": {
      "displayName": "Rongchen Guo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09052573741224419100"
     },
     "user_tz": 300
    },
    "id": "1tzZu8HAlzBQ",
    "outputId": "df215675-fd9c-464f-8687-91714b04c9d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[681 816 241 917 311]], shape=(1, 5), dtype=int64)\n",
      "['None', 'None', 'None', 'None', 'Device']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"I need of an actuator\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv1Ixaa2lzBQ"
   },
   "source": [
    "## Metrics calculation\n",
    "\n",
    "Here is a function to calculate the metrics. The function calculates F1 score for the\n",
    "overall NER dataset as well as individual scores for each NER tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52594\n",
      "52594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Device       0.92      0.91      0.92      1162\n",
      "        None       0.99      1.00      0.99     49241\n",
      "    Resource       0.96      0.87      0.91      1604\n",
      "     Service       0.89      0.67      0.76       587\n",
      "\n",
      "    accuracy                           0.99     52594\n",
      "   macro avg       0.94      0.86      0.90     52594\n",
      "weighted avg       0.99      0.99      0.99     52594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        # print(output)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        # print(predictions)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "        # print(predictions)\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "        # print(true_tag_ids)\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "        # print(true_tag_ids.shape)\n",
    "        # print(predicted_tag_ids.shape)\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "    print(len(all_true_tag_ids))\n",
    "    print(len(all_predicted_tag_ids))\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "#     print(predicted_tags)\n",
    "#     print(real_tags)\n",
    "\n",
    "    # evaluate(real_tags, predicted_tags)\n",
    "    print(classification_report(real_tags, predicted_tags))\n",
    "\n",
    "\n",
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"\"\"\n",
    "As a Doctor, I want to the system to notify me about abnormality in sensor readings.\n",
    "As a Doctor, I want to store in a database any abnormality in breathing of the patient.\n",
    "As an Admin, I want to remove doctors and patients details of the MySQL.\n",
    "As an Admin, I want to my iphone to alert me about some DDOS attacks.\n",
    "As a patient, I want to upload a specific image and video of the patient.\n",
    "\"\"\"\n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(sentence)\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'Device', 'None']\n",
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'Resource', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'Resource']\n",
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'Device', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Sample inference using the trained model\n",
    "sentences=\"\"\"\n",
    "As a Doctor, I want to the system to notify me about abnormality in sensor readings.\n",
    "As a Doctor, I want to store in a database any abnormality in breathing of the patient.\n",
    "As an Admin, I want to remove doctors and patients details of the MySQL.\n",
    "As an Admin, I want to my iphone to alert me about some DDOS attacks.\n",
    "As a patient, I want to upload a specific image and video of the patient.\n",
    "\"\"\"\n",
    "for sentence in sentences.split('.'):\n",
    "    sample_input = tokenize_and_convert_to_ids(sentence)\n",
    "    sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "    output = ner_model.predict(sample_input)\n",
    "    prediction = np.argmax(output, axis=-1)[0]\n",
    "    prediction = [mapping[i] for i in prediction]\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[681 816 241 917 186]], shape=(1, 5), dtype=int64)\n",
      "['None', 'None', 'None', 'None', 'Device']\n"
     ]
    }
   ],
   "source": [
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"I need of an sensor\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ner_transformers_new.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
